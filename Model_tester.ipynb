{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81da24b",
   "metadata": {},
   "source": [
    "# Model Tester\n",
    "<img src=\"https://www.teamly.com/blog/wp-content/uploads/2022/01/What-is-artificial-intelligence-in-project-management.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065dccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to subpress warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# The OS module in Python provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# Matplotlib is a data visualization and graphical plotting library for Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is alse a data visualization and graphical plotting library for Python.\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "# used to display markdown,image,control (frontend utilities)\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "# computer vision library\n",
    "import cv2\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5f5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=r\"D:/Image_datasets/asl_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b97157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class handDetector:\n",
    "    def __init__(self, staticImageMode=False, maxNumHands=2, minDetectionConfidence=0.5, trackCon=0.5):\n",
    "        self.results = None\n",
    "        self.staticImageMode = staticImageMode\n",
    "        self.maxNumberHands = maxNumHands\n",
    "        self.minDetectionConfidence = minDetectionConfidence\n",
    "        self.trackCon = trackCon\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(\n",
    "            static_image_mode=self.staticImageMode,\n",
    "            max_num_hands=self.maxNumberHands,\n",
    "            min_detection_confidence=self.minDetectionConfidence,\n",
    "            min_tracking_confidence=self.trackCon)\n",
    "\n",
    "    def findHands(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "        # print(results.multi_hand_landmarks)\n",
    "        # for rect in self.results.hand_rects:\n",
    "        #     print(rect)\n",
    "\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handLms in self.results.multi_hand_landmarks:\n",
    "                if draw:\n",
    "                    self.mp_drawing.draw_landmarks(img, handLms,\n",
    "                                                   self.mpHands.HAND_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, maxHandNo=1, draw=False):\n",
    "        mainlmlist = []\n",
    "        handsType = []\n",
    "        # handtype=[0,0]\n",
    "        if self.results.multi_handedness:\n",
    "            for hand in self.results.multi_handedness:\n",
    "                # print(hand)\n",
    "                # print(hand.classification)\n",
    "                # print(hand.classification[0])\n",
    "                handType = hand.classification[0].label\n",
    "#                 print(handType)\n",
    "                handsType.append(handType)\n",
    "\n",
    "        # print(len(self.results.multi_hand_landmarks[0]))\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for myHand in self.results.multi_hand_landmarks:\n",
    "                lmList = []\n",
    "                if self.results.multi_hand_landmarks:\n",
    "\n",
    "                    for pid, lm in enumerate(myHand.landmark):\n",
    "                        # print(id, lm)\n",
    "                        h, w, c = img.shape\n",
    "                        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                        # print(id, cx, cy)\n",
    "                        lmList.append([pid, cx, cy])\n",
    "                        if draw:\n",
    "                            cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "                mainlmlist.append(lmList)\n",
    "        return mainlmlist, handsType\n",
    "\n",
    "\n",
    "\n",
    "def predictSign(test,model):\n",
    "#     print(test.shape)\n",
    "    y_pred=model.predict(test)\n",
    "    y_pred_labels=[unique_sign[np.argmax(i)] for i in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b71b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackHand(handDetectorModel,DPath,relativePath):\n",
    "    frame=cv2.imread(os.path.join(DPath,relativePath))\n",
    "    frame=cv2.resize(frame, (400, 400))\n",
    "    clone = handDetectorModel.findHands(frame.copy())\n",
    "    mainlmList, handsType = handDetectorModel.findPosition(clone, draw=False)\n",
    "    flattenedList=[]\n",
    "#     print(clone)\n",
    "#     print(mainlmList)\n",
    "    if len(mainlmList)==0:\n",
    "        return \"empty\",[]\n",
    "    \n",
    "    for keypoint in mainlmList[0]:\n",
    "        flattenedList.append(keypoint[1])\n",
    "        flattenedList.append(keypoint[2])\n",
    "    return handsType[0],flattenedList\n",
    "\n",
    "\n",
    "def Predict(relativePath,model,handDetectorModel,org):\n",
    "    start_time = time.time()\n",
    "    handType,pointslist=trackHand(handDetectorModel,PATH, relativePath)\n",
    "    if handType==\"empty\":\n",
    "        print(\"image : {} -> hand not found -> Aborted\".format(relativePath))\n",
    "        return None\n",
    "    if handType==\"Right\":\n",
    "        pointslist+=[1,0]\n",
    "    else:\n",
    "        pointslist+=[0,1]\n",
    "    \n",
    "    data=np.array(pointslist)\n",
    "    df = pd.DataFrame([data])\n",
    "    pred=predictSign(df,model)[0]\n",
    "    print(\"image : {} | predicted {} | original : {}\".format(relativePath,pred,str(org)))\n",
    "    exeTime=time.time()-start_time\n",
    "    print(\"Execution time :{}ms\".format(round(exeTime*100,2)))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15a9c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : 8\\hand1_8_dif_seg_5_cropped.jpeg | predicted 8 | original : 8\n",
      "Execution time :137.13ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./model.h5')\n",
    "unique_sign=[]\n",
    "with (open(\"test_data.pkl\", \"rb\")) as openfile:\n",
    "        try:\n",
    "            test_object=pickle.load(openfile)\n",
    "        except EOFError as e:\n",
    "            print(\"Error : \",e)\n",
    "unique_sign=test_object[\"unique_sign\"]\n",
    "handDetectorModel=handDetector()\n",
    "    \n",
    "Predict(r\"8\\hand1_8_dif_seg_5_cropped.jpeg\",model,handDetectorModel,\"8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7d8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : t/hand1_t_bot_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :51.42ms\n",
      "image : t/hand1_t_bot_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :52.54ms\n",
      "image : t/hand1_t_bot_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :50.17ms\n",
      "image : t/hand1_t_bot_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :50.02ms\n",
      "image : t/hand1_t_bot_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :57.75ms\n",
      "image : t/hand1_t_dif_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :51.61ms\n",
      "image : t/hand1_t_dif_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :52.2ms\n",
      "image : t/hand1_t_dif_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :51.89ms\n",
      "image : t/hand1_t_dif_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :49.32ms\n",
      "image : t/hand1_t_dif_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :53.51ms\n",
      "image : t/hand1_t_left_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :55.13ms\n",
      "image : t/hand1_t_left_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :50.75ms\n",
      "image : t/hand1_t_left_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :63.0ms\n",
      "image : t/hand1_t_left_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :53.12ms\n",
      "image : t/hand1_t_left_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :54.47ms\n",
      "image : t/hand1_t_right_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :50.76ms\n",
      "image : t/hand1_t_right_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :41.42ms\n",
      "image : t/hand1_t_right_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :40.05ms\n",
      "image : t/hand1_t_right_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :36.82ms\n",
      "image : t/hand1_t_right_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :42.14ms\n",
      "image : t/hand1_t_top_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.58ms\n",
      "image : t/hand1_t_top_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.18ms\n",
      "image : t/hand1_t_top_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.06ms\n",
      "image : t/hand1_t_top_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.81ms\n",
      "image : t/hand1_t_top_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :34.09ms\n",
      "image : t/hand2_t_bot_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :34.74ms\n",
      "image : t/hand2_t_bot_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.05ms\n",
      "image : t/hand2_t_bot_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :40.46ms\n",
      "image : t/hand2_t_bot_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.36ms\n",
      "image : t/hand2_t_bot_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.5ms\n",
      "image : t/hand2_t_left_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :35.21ms\n",
      "image : t/hand2_t_left_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.05ms\n",
      "image : t/hand2_t_left_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :36.51ms\n",
      "image : t/hand2_t_left_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :35.0ms\n",
      "image : t/hand2_t_left_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.16ms\n",
      "image : t/hand2_t_right_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :40.77ms\n",
      "image : t/hand2_t_right_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :34.0ms\n",
      "image : t/hand2_t_right_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.61ms\n",
      "image : t/hand2_t_right_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.69ms\n",
      "image : t/hand2_t_right_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.9ms\n",
      "image : t/hand2_t_top_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.11ms\n",
      "image : t/hand2_t_top_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.12ms\n",
      "image : t/hand2_t_top_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.65ms\n",
      "image : t/hand2_t_top_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :38.32ms\n",
      "image : t/hand2_t_top_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.59ms\n",
      "image : t/hand3_t_dif_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.75ms\n",
      "image : t/hand3_t_dif_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.58ms\n",
      "image : t/hand3_t_dif_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.57ms\n",
      "image : t/hand3_t_dif_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.11ms\n",
      "image : t/hand3_t_dif_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.52ms\n",
      "image : t/hand4_t_bot_seg_1_cropped.jpeg | predicted a | original : t\n",
      "Execution time :32.58ms\n",
      "image : t/hand4_t_bot_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :40.94ms\n",
      "image : t/hand4_t_bot_seg_3_cropped.jpeg | predicted t | original : t\n",
      "Execution time :31.27ms\n",
      "image : t/hand4_t_bot_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :35.6ms\n",
      "image : t/hand4_t_bot_seg_5_cropped.jpeg | predicted a | original : t\n",
      "Execution time :31.8ms\n",
      "image : t/hand5_t_bot_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :30.15ms\n",
      "image : t/hand5_t_bot_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.6ms\n",
      "image : t/hand5_t_bot_seg_3_cropped.jpeg | predicted s | original : t\n",
      "Execution time :33.95ms\n",
      "image : t/hand5_t_bot_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.29ms\n",
      "image : t/hand5_t_bot_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :41.94ms\n",
      "image : t/hand5_t_dif_seg_1_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.45ms\n",
      "image : t/hand5_t_dif_seg_2_cropped.jpeg | predicted t | original : t\n",
      "Execution time :32.89ms\n",
      "image : t/hand5_t_dif_seg_3_cropped.jpeg | predicted s | original : t\n",
      "Execution time :35.28ms\n",
      "image : t/hand5_t_dif_seg_4_cropped.jpeg | predicted t | original : t\n",
      "Execution time :35.11ms\n",
      "image : t/hand5_t_dif_seg_5_cropped.jpeg | predicted t | original : t\n",
      "Execution time :33.14ms\n",
      "\n",
      "\n",
      "-----| Evaluation Result |-----\n",
      "Total : 65 \n",
      "Right : 65 \n",
      "Wrong : 0\n",
      "Skipped : 0\n",
      "Accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "labeldirs=[\"t\"]\n",
    "right=0\n",
    "wrong=0\n",
    "skipped=0\n",
    "for labeldir in labeldirs:\n",
    "    for filepath in os.listdir(os.path.join(PATH,labeldir)):\n",
    "        relativePath=labeldir+\"/\"+filepath\n",
    "        feed=Predict(relativePath,model,handDetectorModel,labeldir)\n",
    "        if feed==None:\n",
    "            skipped+=1\n",
    "            continue\n",
    "        elif feed:\n",
    "            right+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "\n",
    "print(\"\\n\\n-----| Evaluation Result |-----\")\n",
    "print(\"Total : {} \\nRight : {} \\nWrong : {}\".format(right+wrong,right,wrong))\n",
    "print(\"Skipped :\",skipped)\n",
    "print(\"Accuracy : {}\".format(right/(right+wrong)*100,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "23ca6423",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149b9a58",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
