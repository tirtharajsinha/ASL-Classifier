{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "065dccc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# to subpress warning\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "# The OS module in Python provides functions for interacting with the operating system.\n",
    "import os\n",
    "\n",
    "# Matplotlib is a data visualization and graphical plotting library for Python.\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# seaborn is alse a data visualization and graphical plotting library for Python.\n",
    "import seaborn as sn\n",
    "import numpy as np\n",
    "# used to display markdown,image,control (frontend utilities)\n",
    "from IPython.display import display, clear_output\n",
    "import pandas as pd\n",
    "# computer vision library\n",
    "import cv2\n",
    "import pickle\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import mediapipe as mp\n",
    "\n",
    "import time\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5f5c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH=r\"D:/Image_datasets/asl_dataset\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4b97157f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class handDetector:\n",
    "    def __init__(self, staticImageMode=False, maxNumHands=2, minDetectionConfidence=0.5, trackCon=0.5):\n",
    "        self.results = None\n",
    "        self.staticImageMode = staticImageMode\n",
    "        self.maxNumberHands = maxNumHands\n",
    "        self.minDetectionConfidence = minDetectionConfidence\n",
    "        self.trackCon = trackCon\n",
    "        self.mp_drawing = mp.solutions.drawing_utils\n",
    "        self.mp_drawing_styles = mp.solutions.drawing_styles\n",
    "        self.mpHands = mp.solutions.hands\n",
    "        self.hands = self.mpHands.Hands(\n",
    "            static_image_mode=self.staticImageMode,\n",
    "            max_num_hands=self.maxNumberHands,\n",
    "            min_detection_confidence=self.minDetectionConfidence,\n",
    "            min_tracking_confidence=self.trackCon)\n",
    "\n",
    "    def findHands(self, img, draw=True):\n",
    "        imgRGB = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        self.results = self.hands.process(imgRGB)\n",
    "        # print(results.multi_hand_landmarks)\n",
    "        # for rect in self.results.hand_rects:\n",
    "        #     print(rect)\n",
    "\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for handLms in self.results.multi_hand_landmarks:\n",
    "                if draw:\n",
    "                    self.mp_drawing.draw_landmarks(img, handLms,\n",
    "                                                   self.mpHands.HAND_CONNECTIONS)\n",
    "        return img\n",
    "\n",
    "    def findPosition(self, img, maxHandNo=1, draw=False):\n",
    "        mainlmlist = []\n",
    "        handsType = []\n",
    "        # handtype=[0,0]\n",
    "        if self.results.multi_handedness:\n",
    "            for hand in self.results.multi_handedness:\n",
    "                # print(hand)\n",
    "                # print(hand.classification)\n",
    "                # print(hand.classification[0])\n",
    "                handType = hand.classification[0].label\n",
    "#                 print(handType)\n",
    "                handsType.append(handType)\n",
    "\n",
    "        # print(len(self.results.multi_hand_landmarks[0]))\n",
    "        if self.results.multi_hand_landmarks:\n",
    "            for myHand in self.results.multi_hand_landmarks:\n",
    "                lmList = []\n",
    "                if self.results.multi_hand_landmarks:\n",
    "\n",
    "                    for pid, lm in enumerate(myHand.landmark):\n",
    "                        # print(id, lm)\n",
    "                        h, w, c = img.shape\n",
    "                        cx, cy = int(lm.x * w), int(lm.y * h)\n",
    "                        # print(id, cx, cy)\n",
    "                        lmList.append([pid, cx, cy])\n",
    "                        if draw:\n",
    "                            cv2.circle(img, (cx, cy), 15, (255, 0, 255), cv2.FILLED)\n",
    "                mainlmlist.append(lmList)\n",
    "        return mainlmlist, handsType\n",
    "    \n",
    "    \n",
    "    def getBoundedBox(self, lmList):\n",
    "        top, right, bottom, left = sys.maxsize, 0, 0, sys.maxsize\n",
    "        for pts in lmList:\n",
    "            if pts[1] < left:\n",
    "                left = pts[1]\n",
    "            if pts[1] > right:\n",
    "                right = pts[1]\n",
    "\n",
    "            if pts[2] > bottom:\n",
    "                bottom = pts[2]\n",
    "            if pts[2] < top:\n",
    "                top = pts[2]\n",
    "\n",
    "        return top, right, bottom, left\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def predictSign(test,model):\n",
    "#     print(test.shape)\n",
    "    y_pred=model.predict(test)\n",
    "    y_pred_labels=[unique_sign[np.argmax(i)] for i in y_pred]\n",
    "    return y_pred_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9b71b3c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trackHand(handDetectorModel,DPath,relativePath):\n",
    "    frame=cv2.imread(os.path.join(DPath,relativePath))\n",
    "    frame=cv2.resize(frame, (400, 400))\n",
    "    clone = handDetectorModel.findHands(frame.copy())\n",
    "    mainlmList, handsType = handDetectorModel.findPosition(clone, draw=False)\n",
    "    flattenedList=[]\n",
    "#     print(mainlmList)\n",
    "    if len(mainlmList)==0:\n",
    "        return None,[]\n",
    "    for keypoint in mainlmList[0]:\n",
    "        flattenedList.append(keypoint[1])\n",
    "        flattenedList.append(keypoint[2])\n",
    "    return handsType[0],flattenedList\n",
    "\n",
    "\n",
    "def Predict(relativePath,model,org):\n",
    "    start_time = time.time()\n",
    "    handDetectorModel=handDetector()\n",
    "    handType,pointslist=trackHand(handDetectorModel,PATH, relativePath)\n",
    "    if handType==None:\n",
    "        print(\"image : {} -> hand not found -> Aborted\".format(relativePath))\n",
    "        return None\n",
    "    if handType==\"Right\":\n",
    "        pointslist+=[1,0]\n",
    "    else:\n",
    "        pointslist+=[0,1]\n",
    "    \n",
    "    data=np.array(pointslist)\n",
    "    df = pd.DataFrame([data])\n",
    "    pred=predictSign(df,model)[0]\n",
    "    print(\"image : {} | predicted {} | original : {}\".format(relativePath,pred,str(org)))\n",
    "    exeTime=time.time()-start_time\n",
    "    print(\"Execution time :{}ms\".format(round(exeTime*100,2)))\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b15a9c0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : 3\\hand1_3_bot_seg_1_cropped.jpeg | predicted 3 | original : 3\n",
      "Execution time :156.69ms\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = tf.keras.models.load_model('./model.h5')\n",
    "unique_sign=[]\n",
    "with (open(\"test_data.pkl\", \"rb\")) as openfile:\n",
    "        try:\n",
    "            test_object=pickle.load(openfile)\n",
    "        except EOFError as e:\n",
    "            print(\"Error : \",e)\n",
    "unique_sign=test_object[\"unique_sign\"]\n",
    "    \n",
    "Predict(r\"3\\hand1_3_bot_seg_1_cropped.jpeg\",model,\"3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2b7d8ff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image : z/hand1_z_bot_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :72.25ms\n",
      "image : z/hand1_z_bot_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :65.68ms\n",
      "image : z/hand1_z_bot_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :53.99ms\n",
      "image : z/hand1_z_bot_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :49.04ms\n",
      "image : z/hand1_z_bot_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :46.0ms\n",
      "image : z/hand1_z_dif_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.98ms\n",
      "image : z/hand1_z_dif_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.12ms\n",
      "image : z/hand1_z_dif_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :44.9ms\n",
      "image : z/hand1_z_dif_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.94ms\n",
      "image : z/hand1_z_dif_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :46.8ms\n",
      "image : z/hand1_z_left_seg_1_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_left_seg_2_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_left_seg_3_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_left_seg_4_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_left_seg_5_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_right_seg_1_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_right_seg_2_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_right_seg_3_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_right_seg_4_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_right_seg_5_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand1_z_top_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :72.2ms\n",
      "image : z/hand1_z_top_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :50.61ms\n",
      "image : z/hand1_z_top_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :51.21ms\n",
      "image : z/hand1_z_top_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :49.76ms\n",
      "image : z/hand1_z_top_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.8ms\n",
      "image : z/hand2_z_bot_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.21ms\n",
      "image : z/hand2_z_bot_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :50.03ms\n",
      "image : z/hand2_z_bot_seg_3_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand2_z_bot_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.52ms\n",
      "image : z/hand2_z_bot_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :55.5ms\n",
      "image : z/hand2_z_dif_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.72ms\n",
      "image : z/hand2_z_dif_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.3ms\n",
      "image : z/hand2_z_dif_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :49.23ms\n",
      "image : z/hand2_z_dif_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.42ms\n",
      "image : z/hand2_z_dif_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.7ms\n",
      "image : z/hand2_z_left_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.43ms\n",
      "image : z/hand2_z_left_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :50.71ms\n",
      "image : z/hand2_z_left_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.73ms\n",
      "image : z/hand2_z_left_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :46.27ms\n",
      "image : z/hand2_z_left_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :44.51ms\n",
      "image : z/hand2_z_right_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.98ms\n",
      "image : z/hand2_z_right_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :45.41ms\n",
      "image : z/hand2_z_right_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :49.58ms\n",
      "image : z/hand2_z_right_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.99ms\n",
      "image : z/hand2_z_right_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :56.0ms\n",
      "image : z/hand2_z_top_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :46.02ms\n",
      "image : z/hand2_z_top_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.01ms\n",
      "image : z/hand2_z_top_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.99ms\n",
      "image : z/hand2_z_top_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :48.03ms\n",
      "image : z/hand2_z_top_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :46.98ms\n",
      "image : z/hand3_z_dif_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.23ms\n",
      "image : z/hand3_z_dif_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :44.77ms\n",
      "image : z/hand3_z_dif_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :57.35ms\n",
      "image : z/hand3_z_dif_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :47.67ms\n",
      "image : z/hand3_z_dif_seg_5_cropped.jpeg | predicted z | original : z\n",
      "Execution time :51.69ms\n",
      "image : z/hand4_z_bot_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :50.5ms\n",
      "image : z/hand4_z_bot_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :53.9ms\n",
      "image : z/hand4_z_bot_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :56.7ms\n",
      "image : z/hand4_z_bot_seg_4_cropped.jpeg | predicted z | original : z\n",
      "Execution time :52.9ms\n",
      "image : z/hand4_z_bot_seg_5_cropped.jpeg | predicted g | original : z\n",
      "Execution time :47.27ms\n",
      "image : z/hand5_z_bot_seg_1_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand5_z_bot_seg_2_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand5_z_bot_seg_3_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand5_z_bot_seg_4_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand5_z_bot_seg_5_cropped.jpeg -> hand not found -> Aborted\n",
      "image : z/hand5_z_dif_seg_1_cropped.jpeg | predicted z | original : z\n",
      "Execution time :75.02ms\n",
      "image : z/hand5_z_dif_seg_2_cropped.jpeg | predicted z | original : z\n",
      "Execution time :53.18ms\n",
      "image : z/hand5_z_dif_seg_3_cropped.jpeg | predicted z | original : z\n",
      "Execution time :55.12ms\n",
      "image : z/hand5_z_dif_seg_4_cropped.jpeg | predicted 1 | original : z\n",
      "Execution time :49.9ms\n",
      "image : z/hand5_z_dif_seg_5_cropped.jpeg | predicted 1 | original : z\n",
      "Execution time :55.45ms\n",
      "\n",
      "\n",
      "-----| Evaluation Result |-----\n",
      "Total : 54 \n",
      "Right : 54 \n",
      "Wrong : 0\n",
      "Skipped : 16\n",
      "Accuracy : 100.0\n"
     ]
    }
   ],
   "source": [
    "labeldirs=[\"z\"]\n",
    "right=0\n",
    "wrong=0\n",
    "skipped=0\n",
    "for labeldir in labeldirs:\n",
    "    for filepath in os.listdir(os.path.join(PATH,labeldir)):\n",
    "        relativePath=labeldir+\"/\"+filepath\n",
    "        feed=Predict(relativePath,model,labeldir)\n",
    "        if feed==None:\n",
    "            skipped+=1\n",
    "            continue\n",
    "        elif feed:\n",
    "            right+=1\n",
    "        else:\n",
    "            wrong+=1\n",
    "\n",
    "print(\"\\n\\n-----| Evaluation Result |-----\")\n",
    "print(\"Total : {} \\nRight : {} \\nWrong : {}\".format(right+wrong,right,wrong))\n",
    "print(\"Skipped :\",skipped)\n",
    "print(\"Accuracy : {}\".format(right/(right+wrong)*100,2))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1be16a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
